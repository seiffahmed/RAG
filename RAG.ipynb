{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0f36bb-dceb-40a4-9955-2547591536b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('sampled_jobs.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Clean description column\n",
    "df['description'] = df['description'].apply(remove_html_tags)\n",
    "\n",
    "# Drop rows with empty descriptions\n",
    "df = df[df['description'].str.strip().astype(bool)]\n",
    "\n",
    "# Concatenate job_title with cleaned description\n",
    "df['Docs'] = df['job_title'] + ': ' + df['description']\n",
    "df.drop(['job_title', 'description', 'career_level'], axis=1, inplace=True)\n",
    "\n",
    "# Example of saving cleaned data to a new CSV\n",
    "df.to_csv('cleaned_sampled_jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db6d61-4566-4a28-82ad-57001fcf4ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seiff\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\seiff\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Sentence Transformers model\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Embedding job descriptions\n",
    "embeddings = model.encode(df['Docs'].tolist(), convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5d3db-a5bf-453b-b392-2c7750c7c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from elasticsearch_dsl import Index, Document, Text, Keyword, Object, Float\n",
    "import numpy as np\n",
    "\n",
    "# Example Elasticsearch setup \n",
    "es = Elasticsearch()\n",
    "\n",
    "# Create an Elasticsearch index\n",
    "index_name = 'jobs_index'\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name)\n",
    "\n",
    "# Define Elasticsearch document mapping\n",
    "class JobDocument(Document):\n",
    "    title = Text()\n",
    "    description = Text()\n",
    "    embeddings = Object()\n",
    "\n",
    "    class Index:\n",
    "        name = index_name\n",
    "\n",
    "# Function to bulk index documents into Elasticsearch\n",
    "def bulk_index_documents(df, embeddings):\n",
    "    actions = []\n",
    "    for i, row in df.iterrows():\n",
    "        doc = JobDocument(\n",
    "            title=row['job_title'],\n",
    "            description=row['description'],\n",
    "            embeddings=embeddings[i].tolist()\n",
    "        )\n",
    "        actions.append(doc.to_dict(include_meta=True))\n",
    "\n",
    "    bulk(es, actions)\n",
    "\n",
    "# Index documents with embeddings into Elasticsearch\n",
    "bulk_index_documents(df, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfaf059-d5ff-4e0a-b74b-185a3f280d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load Phi-3 - Mini 4k Instruct model and tokenizer\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate advice \n",
    "def generate_advice(query):\n",
    "    inputs = tokenizer.encode(query, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=500, num_return_sequences=1, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "user_query = \"How to become a Machine Learning Engineer?\"\n",
    "advice = generate_advice(user_query)\n",
    "print(advice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c8d17-4669-4c29-b01b-e54c3b983228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system(user_query):\n",
    "    # Retrieve relevant documents from Elasticsearch\n",
    "    query_vector = model.encode([user_query], convert_to_tensor=True)\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embeddings\": {\n",
    "                    \"vector\": query_vector.numpy().tolist(),\n",
    "                    \"k\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    search_results = es.search(index=index_name, body=query)['hits']['hits']\n",
    "\n",
    "    # Extract retrieved document IDs\n",
    "    retrieved_ids = [hit['_id'] for hit in search_results]\n",
    "\n",
    "    # Generate personalized advice\n",
    "    recommendations = []\n",
    "    for doc_id in retrieved_ids:\n",
    "        description = df.loc[df.index == int(doc_id), 'Docs'].values[0]\n",
    "        advice = generate_advice(description)\n",
    "        recommendations.append(advice)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example usage of the RAG system\n",
    "user_query = \"How to become a Machine Learning Engineer?\"\n",
    "recommendations = rag_system(user_query)\n",
    "for idx, advice in enumerate(recommendations):\n",
    "    print(f\"Recommendation {idx + 1}: {advice}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112915b-60f7-416b-97c3-c2ca1c565704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_system(queries):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        recommendations = rag_system(query)\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'recommendations': recommendations\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Example test queries for evaluation\n",
    "test_queries = [\n",
    "    \"How to become a Data Scientist?\",\n",
    "    \"Skills needed for a Software Engineer role\",\n",
    "    \"Career advice for aspiring Project Managers\"\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_rag_system(test_queries)\n",
    "for result in evaluation_results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    for idx, advice in enumerate(result['recommendations']):\n",
    "        print(f\"Recommendation {idx + 1}: {advice}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589c82d-566b-4c59-82d8-8bd16b55be59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
